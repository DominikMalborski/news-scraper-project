{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f68b6-e6c0-4a6e-ab69-c0caf7213430",
   "metadata": {},
   "source": [
    "# üßæ AI News Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d23cd4-3cf7-453c-b472-a01675dd234c",
   "metadata": {},
   "source": [
    "This project is a simple web application that scrapes a news website and generates a short briefing using `OpenAI's GPT-4o`model.\n",
    "It works by:\n",
    "\n",
    "- Scraping the homepage of a given news site,\n",
    "- Extracting article titles and links,\n",
    "- Downloading full content of selected articles,\n",
    "- Sending the text to GPT-4o with a custom prompt,\n",
    "- Displaying the result in a readable format using Gradio.\n",
    "\n",
    "The goal is to create a fast and simple way to get summarized, AI-generated news briefings ‚Äî useful for analysts, researchers, or anyone who wants quick insight into current events.\n",
    "\n",
    "‚öôÔ∏è Technologies used:\n",
    "- Python (requests, BeautifulSoup)\n",
    "- OpenAI API (GPT-4o)\n",
    "- Gradio (UI)\n",
    "- dotenv (API key handling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cdf95-537d-4aa1-ab8a-8952cbbffe83",
   "metadata": {},
   "source": [
    "## üì¶ 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b89ada-aad2-4880-beee-cb2570803076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8474b-03d9-490d-9788-f9a27c16d389",
   "metadata": {},
   "source": [
    "## üß© Loading API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2f5bdb-16b8-4029-96af-8f09651a38cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a7278-d86f-4d9a-bfd3-1eccb0cf7fb2",
   "metadata": {},
   "source": [
    "## üß± 2. Website Scraper Class\n",
    "\n",
    "This section defines the `Website` class, which is responsible for downloading and parsing the content of a given news website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69451867-80a6-47af-863f-636867bd4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    description: str\n",
    "    text: str\n",
    "    body: str\n",
    "    links: List[str]\n",
    "    link_titles: List[tuple]\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string.strip() if soup.title else \"No title found\"\n",
    "        meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        self.description = meta[\"content\"].strip() if meta and meta.get(\"content\") else \"\"\n",
    "        self.text = self.extract_article_text(soup)\n",
    "        self.links = []\n",
    "        self.link_titles = []\n",
    "\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            title = link.get_text(strip=True)\n",
    "\n",
    "            if href.startswith(\"http\"):\n",
    "                self.links.append(href)\n",
    "                if title:\n",
    "                    self.link_titles.append((title, href))\n",
    "\n",
    "    def extract_article_text(self, soup):\n",
    "        selectors = [\n",
    "            \"article p\",\n",
    "            \"div.article-content p\",\n",
    "            \"div.wp-content-text-raw p\",\n",
    "            \"p\"\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            paragraphs = soup.select(selector)\n",
    "            if paragraphs:\n",
    "                break\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "        text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        return text.strip()\n",
    "\n",
    "    def get_content(self):\n",
    "        return (\n",
    "            f\"Webpage Title:\\n{self.title}\\n\\n\"\n",
    "            f\"Meta Description:\\n{self.description}\\n\\n\"\n",
    "            f\"Article Content:\\n{self.text}\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec222d7-d9a2-44f0-9490-d845d44eff62",
   "metadata": {},
   "source": [
    "## üß† 3. System Prompt for Link Extraction\n",
    "\n",
    "This prompt is sent to the GPT model as a **system message**, telling it how to behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57bc9733-c4c9-42b7-a8c0-f05c85129ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = \"\"\"\n",
    "\n",
    "You are to collect from the given informational website (its main page) the titles and links to articles. Format the collected data as a JSON structure like the example below:\n",
    "\n",
    "{\n",
    "  \"links\": [\n",
    "    {\"title\": \"Headline text\", \"url\": \"https://full.url/article1\"},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabecd3-a48f-4ec3-91a4-5be05b057418",
   "metadata": {},
   "source": [
    "## ‚úèÔ∏è 4. User Prompt Generator\n",
    "\n",
    "This function builds the **user message** (user prompt) that is sent to the GPT model, together with the system prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddbcd1d8-11f3-4c57-87df-a4a1aee443bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links_user_prompt(website):\n",
    "    user_prompt = f\"Here are the links found on the website: {website.url}\\n\"\n",
    "    user_prompt += \"Each line includes the article title and its full URL.\\n\"\n",
    "    user_prompt += \"Please extract only the most important news links based on the system instructions.\\n\\n\"\n",
    "\n",
    "    if hasattr(website, 'link_titles') and website.link_titles:\n",
    "        for title, link in website.link_titles:\n",
    "            if len(title) > 5:\n",
    "                user_prompt += f\"- {title} ‚Üí {link}\\n\"\n",
    "    else:\n",
    "        for link in website.links:\n",
    "            user_prompt += f\"- {link}\\n\"\n",
    "\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178b848-b6c3-4212-b0ab-47dc002f61f7",
   "metadata": {},
   "source": [
    "## üîó 5. Fetching Clean Article Links via GPT\n",
    "\n",
    "This function sends the parsed website data to the OpenAI GPT model and returns a clean list of important article links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1168495c-69c6-4a12-b090-8c7df42b9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            {\"role\":\"system\", \"content\":link_system_prompt},\n",
    "            {\"role\":\"user\", \"content\":get_news_links_user_prompt(website)}\n",
    "        ],\n",
    "        response_format={\"type\":\"json_object\"}\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea51fa9-7533-4f81-9439-686415a17ce6",
   "metadata": {},
   "source": [
    "### üîç Function: `get_all_details(url, max_articles=10)`\n",
    "\n",
    "This function orchestrates the full scraping and summarization pipeline.\n",
    "\n",
    "**Steps:**\n",
    "1. Downloads and parses the landing page using the `Website` class,\n",
    "2. Sends extracted links to GPT (via `get_links`) to identify key articles,\n",
    "3. Iteratively fetches the full content of up to `max_articles` articles,\n",
    "4. Combines all text into a structured markdown report.\n",
    "\n",
    "The output is truncated at 65,000 characters to comply with token/processing limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5092ba9-bc09-447a-9ec1-012f95026973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_all_details(url, max_articles=10):\n",
    "    result = \"Landing page:\\n\"\n",
    "\n",
    "    try:\n",
    "        result += Website(url).get_content()\n",
    "    except Exception as e:\n",
    "        result += f\"\\n Could not fetch landing page: {e}\\n\"\n",
    "    links_data = get_links(url)\n",
    "    if isinstance(links_data, str):\n",
    "        try:\n",
    "            parsed = json.loads(links_data)\n",
    "            links = parsed.get(\"links\", [])\n",
    "        except Exception as e:\n",
    "            print(f\" JSON decode error: {e}\")\n",
    "            links = []\n",
    "    else:\n",
    "        links = links_data.get(\"links\", [])\n",
    "\n",
    "    print(f\"Parsed links count: {len(links)}\")\n",
    "\n",
    "    for link in links[:max_articles]:\n",
    "        title = link.get(\"title\", \"No title\")\n",
    "        article_url = link.get(\"url\")\n",
    "\n",
    "        if not article_url:\n",
    "            continue\n",
    "\n",
    "        print(f\"Scraping article: {title} ‚Üí {article_url}\")\n",
    "\n",
    "        try:\n",
    "            result += f\"\\n\\n\\n---\\n\\n\\n\"\n",
    "            result += f\"### {title}\\n\"\n",
    "            result += Website(article_url).get_content()\n",
    "            result += f\"\\n\\n[üìñ Read full article ‚Üí]({article_url})\\n\"\n",
    "        except Exception as e:\n",
    "            result += f\"{title} (Error: {e})\\n\"\n",
    "\n",
    "    return result[:65000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595f5354-ec7e-414f-a280-76b65bc2a9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#display(Markdown(get_all_details(\"https://businessinsider.com.pl\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171179ea-112d-4625-a0ee-4531a899d8e7",
   "metadata": {},
   "source": [
    "## üß† 7. System Prompt: News Analyst Assistant\n",
    "\n",
    "This system prompt defines the role and tone of the GPT model when generating the final news summary.\n",
    "\n",
    "### üìù Purpose:\n",
    "Instructs the model to act like a **personal news analyst** ‚Äî not just summarizing facts, but explaining their importance and implications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91b1bbc4-e3f8-44f0-97d5-93ee1e2914ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are my personal information assistant and news analyst. Your task is to review the scraped content from a news homepage and its linked articles.\n",
    "\n",
    "Based on that material, produce a clear, insightful, and human-sounding summary of recent developments in Poland and globally. Write as if you‚Äôre speaking directly to me ‚Äî explaining what‚Äôs happening, why it matters, and what possible consequences these events might have.\n",
    "\n",
    "Please include:\n",
    "- A summary of key **facts and developments** (from the articles),\n",
    "- An analysis of their **significance and potential impact** (political, economic, social, etc.),\n",
    "- Your own **interpretations or reflections**, when appropriate.\n",
    "\n",
    "Avoid robotic summaries. Speak in your own words ‚Äî like a smart, well-informed advisor helping me stay ahead of the curve.\n",
    "\n",
    "Use **Markdown format** (headings, bullet points, paragraphs).\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5522f-3f2a-4eb4-a7c5-764436c915f5",
   "metadata": {},
   "source": [
    "## üìù 8. User Prompt for Final Briefing\n",
    "\n",
    "This function generates the full **user message** for the GPT model, combining:\n",
    "\n",
    "- High-level instructions for summarization and analysis,\n",
    "- The full text scraped from the homepage and selected articles.\n",
    "\n",
    "### üéØ Purpose:\n",
    "To give the model enough context to produce a **clear, structured, and insightful news briefing**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6125e526-157f-45d2-871b-79fb9f975438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_user_prompt(url):\n",
    "    user_prompt = f\"## Source: {url}\\n\\n\"\n",
    "    user_prompt += (\n",
    "    f\"You are reviewing the main content and linked articles from the website: **{url}**.\\n\\n\"\n",
    "    \"Below is the full scraped text from the homepage, followed by selected full articles.\\n\"\n",
    "    \"Your goal is to produce an intelligent, insightful, and well-structured market briefing.\\n\\n\"\n",
    "    \"Please:\\n\"\n",
    "    \"- Identify key developments and summarize them clearly,\\n\"\n",
    "    \"- Highlight relevant trends and explain their potential impact,\\n\"\n",
    "    \"- Reflect on possible political, economic, or societal consequences,\\n\"\n",
    "    \"- Use your own judgment and speak in your own words, like a smart advisor.\\n\\n\"\n",
    "    \"Write in clear, professional English ‚Äî as if advising an informed investor.\\n\"\n",
    "    \"Format the response in **Markdown** (with headings, bullets, etc.).\"\n",
    "    )\n",
    "    user_prompt += get_all_details(url)\n",
    "    user_prompt = user_prompt[:20_000]\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d58f2d-3bea-4be0-bf2b-a7907e734ed1",
   "metadata": {},
   "source": [
    "## üì§ 9. Streaming the AI-Generated Briefing\n",
    "\n",
    "This function sends the full prompt to GPT and **streams the generated response** in real time.\n",
    "\n",
    "### üîÑ What it does:\n",
    "- Combines the `system_prompt` (defines the model‚Äôs role) and the `user_prompt` (full content + instructions),\n",
    "- Sends both to GPT using the `stream=True` parameter,\n",
    "- Receives the response in small chunks,\n",
    "- Reconstructs the final output piece by piece,\n",
    "- Cleans up unnecessary formatting (e.g. code blocks),\n",
    "- Uses `yield` to display results live ‚Äî ideal for web interfaces like Gradio.\n",
    "\n",
    "This is the final step where the AI-generated market briefing is produced and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f357a97-a850-4f3c-806e-02c49a4c3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_news_summary(url):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': get_brochure_user_prompt(url)}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or ''\n",
    "        response += content\n",
    "        cleaned = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        yield cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73833955-7cf7-42f2-8cbd-6c35249bf293",
   "metadata": {},
   "source": [
    "## üñ•Ô∏è 10. User Interface (Gradio App)\n",
    "\n",
    "This section creates a simple web interface using **Gradio**, allowing users to generate AI-based news briefings in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab07edb6-480c-460f-8e22-4bc6d6a9e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "custom_css = \"\"\"\n",
    "#brief-box {\n",
    "    background-color: #1f1f1f;\n",
    "    color: white;\n",
    "    padding: 20px;\n",
    "    font-size: 18px;\n",
    "    border-radius: 12px;\n",
    "    min-height: 400px;\n",
    "    max-height: 600px;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "\"\"\"\n",
    "with gr.Blocks(css=custom_css, title=\"News Generator\") as ui:\n",
    "    gr.Markdown(\"NewsGrid News Generator\")\n",
    "    gr.Markdown(\"Enter a news URL and get a real-time AI-generated news report based content.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            url_input = gr.Textbox(label=\"üîó News Website URL\", placeholder=\"https://example.com\", value=\"https://businessinsider.com.pl\")\n",
    "            generate_btn = gr.Button(\"‚ö° Generate Briefing\")\n",
    "            clear_btn = gr.Button(\"üßπ Clear\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            output_box = gr.Markdown(\"Waiting for URL...\", elem_id=\"brief-box\",)\n",
    "\n",
    "    generate_btn.click(fn=stream_news_summary, inputs=[url_input], outputs=[output_box])\n",
    "    clear_btn.click(fn=lambda: \"Waiting for URL...\", outputs=[output_box], queue=False)\n",
    "\n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf75b0c-4d15-45b2-9ede-7e7196458924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415bd5a2-4723-4cbf-bf36-0eb76a8b1138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915882c-d26a-4ae6-9ed0-f4fe84364a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34d165-6f7f-48be-b0b0-7a4918b60cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a9a48-fc82-41c4-8947-ab9b76c18f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f6487-5430-4d43-93fc-1ac2d1a9c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2427d-f756-451b-8e57-6c64a0c1d941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428196a1-3436-4c44-add1-bf6fb38ef554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5c020-0b4c-4f66-af8c-555e058b7f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26a35d-e9c2-4ef6-b7c9-9489419397eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d0e1d-9693-4b9f-ac1e-2dbc0313a425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f7ec8-9257-48bb-941d-8730eea334ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeaaced-f9e0-40bc-9e0b-59c244cf3ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34050fda-8807-4746-b77c-0fe7b5d24eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
